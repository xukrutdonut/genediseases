# Gu√≠a de Configuraci√≥n Completa

## Base de Datos de Gen√©tica Cl√≠nica

Este proyecto combina informaci√≥n de dos fuentes principales:
1. **GeneReviews** (NCBI) - Reviews de enfermedades gen√©ticas
2. **Oxford Desk Reference: Clinical Genetics and Genomics** - Libro de referencia completo

---

## üöÄ Inicio R√°pido

### 1. Instalaci√≥n de Dependencias

```bash
# Dependencias del sistema (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install tesseract-ocr poppler-utils

# Dependencias de Node.js
npm install
```

### 2. Recolecci√≥n de Datos

```bash
# Opci√≥n A: Todo en uno (recomendado)
npm run setup

# Opci√≥n B: Paso a paso
npm run scrape        # Scraping de GeneReviews (~5-10 minutos)
npm run process-pdf   # Procesamiento del PDF Oxford (~2-3 minutos)
```

### 3. Iniciar Servidor

```bash
# Terminal 1: Iniciar servidor
npm start

# Terminal 2: Cargar datos en la base de datos
curl -X POST http://localhost:3000/api/admin/load-data
curl -X POST http://localhost:3000/api/admin/load-book-data
```

### 4. Acceder a la Aplicaci√≥n

Abrir en el navegador: http://localhost:3000

---

## üìä Datos Procesados

### GeneReviews
- **Fuente**: https://www.ncbi.nlm.nih.gov/books/NBK1116/
- **Contenido**: Reviews estructuradas de enfermedades gen√©ticas
- **Formato**: JSON estructurado con secciones, autores, referencias
- **Ubicaci√≥n**: `database/genereviews-data.json`

### Oxford Clinical Genetics
- **Fuente**: PDF descargado autom√°ticamente
- **Contenido**: 530+ secciones extra√≠das del libro
- **Formato**: Texto completo con estructura de cap√≠tulos
- **Ubicaci√≥n**: `data/pdf_extracted/oxford_genetics_data.json`
- **Tama√±o**: ~54,000 l√≠neas de texto

---

## üîç Uso de la API

### B√∫squeda Global (todas las fuentes)
```bash
curl "http://localhost:3000/api/search/all?q=cystic+fibrosis&limit=10"
```

### B√∫squeda en GeneReviews
```bash
curl "http://localhost:3000/api/search?q=genetics&limit=20"
```

### B√∫squeda en Libros
```bash
curl "http://localhost:3000/api/books/search?q=chromosome&limit=20"
```

### Estad√≠sticas
```bash
curl http://localhost:3000/api/stats
```

---

## üìÅ Estructura de la Base de Datos

### Tabla: book_sections
| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| id | INTEGER | ID √∫nico |
| source | TEXT | Fuente del contenido |
| title | TEXT | T√≠tulo de la secci√≥n |
| content | TEXT | Contenido completo |
| page | INTEGER | N√∫mero de p√°gina |
| section_order | INTEGER | Orden de la secci√≥n |

### FTS (Full Text Search)
- `reviews_fts`: √çndice de b√∫squeda para GeneReviews
- `book_sections_fts`: √çndice de b√∫squeda para libros

---

## üõ†Ô∏è Scripts Disponibles

| Script | Descripci√≥n |
|--------|-------------|
| `npm start` | Iniciar servidor (puerto 3000) |
| `npm run dev` | Modo desarrollo con nodemon |
| `npm run scrape` | Scraping de GeneReviews |
| `npm run process-pdf` | Procesar PDF con extracci√≥n de texto |
| `npm run process-pdf:ocr` | Procesar PDF con OCR (m√°s lento) |
| `npm run setup` | Ejecutar scraping + procesamiento |

---

## üîß Troubleshooting

### Error: tesseract no encontrado
```bash
sudo apt-get install tesseract-ocr tesseract-ocr-eng
```

### Error: pdftotext no encontrado
```bash
sudo apt-get install poppler-utils
```

### Error: out of memory
Reducir tama√±o de batch en `pdf-ocr-processor.js` o usar:
```bash
NODE_OPTIONS='--max-old-space-size=4096' npm run process-pdf
```

### Base de datos bloqueada
```bash
# Detener servidor y reiniciar
pkill -f "node backend/server.js"
npm start
```

---

## üìà M√©tricas del Sistema

### Datos Procesados
- **GeneReviews**: Variable (depende del scraping)
- **Oxford Genetics**: 530 secciones, ~54,000 l√≠neas
- **Base de datos**: ~10-50 MB (SQLite)

### Rendimiento
- **B√∫squeda FTS**: <100ms para consultas t√≠picas
- **Carga de p√°gina**: <50ms
- **Scraping inicial**: 5-15 minutos
- **Procesamiento PDF**: 2-5 minutos

---

## üîê Seguridad y Privacidad

- No se almacenan credenciales
- Datos m√©dicos de dominio p√∫blico
- API REST sin autenticaci√≥n (a√±adir en producci√≥n)
- Rate limiting en el scraper para respetar servidores

---

## üìù Notas Importantes

1. El PDF se descarga autom√°ticamente la primera vez
2. El procesamiento usa extracci√≥n directa (no OCR) por defecto
3. Los datos se mantienen localmente en SQLite
4. La b√∫squeda de texto completo usa SQLite FTS5
5. El scraping respeta los l√≠mites del servidor NCBI

---

## üéØ Pr√≥ximos Pasos

1. Implementar frontend completo con React/Vue
2. A√±adir autenticaci√≥n y control de acceso
3. Implementar cach√© de b√∫squedas
4. A√±adir m√°s fuentes de datos m√©dicos
5. Crear API GraphQL alternativa
6. Implementar exportaci√≥n de datos

---

## üìû Soporte

Para problemas o preguntas, revisar:
- `README.md` - Documentaci√≥n general
- `package.json` - Scripts disponibles
- `backend/server.js` - API endpoints
- `database/database.js` - Esquema de datos
